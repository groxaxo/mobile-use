# Docker Compose for Local/Home Server Deployment
# This configuration is optimized for running mobile-use on a home server,
# Raspberry Pi, NAS, or any always-on local machine.
#
# Features:
# - Persistent volumes for configuration and logs
# - Easy wireless device connection
# - Support for local LLMs via Ollama
# - Optimized for long-running deployments

version: '3.8'

services:
  # Main mobile-use service for wireless/IP connection
  mobile-use:
    image: minitap/mobile-use:latest
    container_name: mobile-use
    hostname: mobile-use-server
    restart: unless-stopped  # Auto-restart on failure, but not after manual stop
    
    # Run as non-root user for security
    user: "mobile-use:mobile-use"
    
    # Environment configuration
    env_file:
      - .env
    environment:
      # Device connection (set your device IP here or via .env)
      ADB_CONNECT_ADDR: "${ADB_CONNECT_ADDR:-}"
      
      # Output paths for results and events
      RESULTS_OUTPUT_PATH: "/home/mobile-use/results.txt"
      EVENTS_OUTPUT_PATH: "/home/mobile-use/events.json"
      
      # Health check settings (adjust for your network)
      MOBILE_USE_HEALTH_RETRIES: "150"
      MOBILE_USE_HEALTH_DELAY: "2"
      
      # Timezone (adjust to your location)
      TZ: "${TZ:-UTC}"
    
    # Volumes for persistence
    volumes:
      # ADB keys (persists device authorization)
      - mobile-use-adb-keys:/home/mobile-use/.android
      
      # Custom LLM configuration
      - ./llm-config.override.jsonc:/app/llm-config.override.jsonc:ro
      
      # Logs and outputs (optional - uncomment to persist)
      # - ./logs:/home/mobile-use/logs
      # - ./results:/home/mobile-use/results
    
    # Network configuration
    # Use host network for easier local LLM access (Ollama)
    # Comment this out if you prefer bridge network
    # network_mode: host
    networks:
      - mobile-use-network
    
    # Resource limits (adjust based on your server capacity)
    deploy:
      resources:
        limits:
          cpus: '2.0'      # Max 2 CPU cores
          memory: 2G       # Max 2GB RAM
        reservations:
          cpus: '0.5'      # Reserve 0.5 CPU
          memory: 512M     # Reserve 512MB RAM
    
    # Healthcheck to ensure service is responsive
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # USB-connected device service (alternative to wireless)
  mobile-use-usb:
    image: minitap/mobile-use:latest
    container_name: mobile-use-usb
    hostname: mobile-use-server-usb
    restart: unless-stopped
    user: "mobile-use:mobile-use"
    
    # Privileged mode required for USB access
    privileged: true
    devices:
      - "/dev/bus/usb:/dev/bus/usb"
    
    env_file:
      - .env
    environment:
      RESULTS_OUTPUT_PATH: "/home/mobile-use/results.txt"
      EVENTS_OUTPUT_PATH: "/home/mobile-use/events.json"
      TZ: "${TZ:-UTC}"
    
    volumes:
      - mobile-use-adb-keys:/home/mobile-use/.android
      - ./llm-config.override.jsonc:/app/llm-config.override.jsonc:ro
    
    networks:
      - mobile-use-network
    
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    
    # By default, don't start this service (use mobile-use instead)
    profiles:
      - usb

  # Optional: Ollama service for local LLM (complete privacy)
  # Uncomment to run Ollama alongside mobile-use
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   hostname: ollama-server
  #   restart: unless-stopped
  #   
  #   # Volumes for model storage
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   
  #   # Expose port for mobile-use to connect
  #   ports:
  #     - "11434:11434"
  #   
  #   networks:
  #     - mobile-use-network
  #   
  #   # Resource limits (adjust for larger models)
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '4.0'
  #         memory: 8G
  #       reservations:
  #         cpus: '2.0'
  #         memory: 4G
  #   
  #   # GPU support (if available)
  #   # Uncomment for NVIDIA GPU support
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

# Networks
networks:
  mobile-use-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# Volumes for persistence
volumes:
  mobile-use-adb-keys:
    driver: local
  # ollama-models:
  #   driver: local

# Usage Examples:
# ================
#
# 1. Start with wireless connection:
#    export ADB_CONNECT_ADDR="192.168.1.100:5555"
#    docker-compose -f docker-compose.local-server.yml run --rm mobile-use "Check my email"
#
# 2. Start with USB connection:
#    docker-compose -f docker-compose.local-server.yml run --rm mobile-use-usb "Check my email"
#
# 3. Run as persistent service:
#    docker-compose -f docker-compose.local-server.yml up -d mobile-use
#
# 4. View logs:
#    docker-compose -f docker-compose.local-server.yml logs -f mobile-use
#
# 5. Stop service:
#    docker-compose -f docker-compose.local-server.yml down
#
# 6. With Ollama (uncomment ollama service above):
#    docker-compose -f docker-compose.local-server.yml up -d ollama
#    # Wait for startup, then pull a model:
#    docker exec ollama ollama pull llama3.1
#    # Configure .env:
#    # OPENAI_BASE_URL=http://ollama:11434/v1
#    # OPENAI_API_KEY=ollama
